[[configs]]
datasets = ["mnli"]
dataset_config_name = ["en"]
device = "cuda"
learning_rate = 0.3
model_name_or_path = "t5-base"
tokenizer_name_or_path = "t5-base"
max_target_length = 128
num_epochs = 10
batch_size = 32
peft_type = "attempt"
attn_method = "sub"
temperature = 2087
prompt_init = "embedding"
prompt_init_embedding = "soft_prompts/ours/mnli.bin"
prompt_embedding_paths = ["soft_prompts/ours/mnli.bin", "soft_prompts/ours/qnli.bin", "soft_prompts/ours/qqp.bin", "soft_prompts/ours/record.bin", "soft_prompts/ours/squad.bin", "soft_prompts/ours/sst2.bin"]
prefix_num=6
task_type = "seq_2_seq_lm"
num_virtual_tokens = 50
n_runs = 3
wandb_project = "attempt_single_ours_experiments"
max_source_length = 256
split_validation_test = true
# max_train_samples = 1000
# max_valid_samples = 500
# max_test_samples = 500
output_dir = "attempt_single_mnli_ours"
warmup_steps = 500
shared_attn = false

[[configs]]
datasets = ["qqp"]
dataset_config_name = ["en"]
device = "cuda"
learning_rate = 0.3
model_name_or_path = "t5-base"
tokenizer_name_or_path = "t5-base"
max_target_length = 128
num_epochs = 10
batch_size = 32
peft_type = "attempt"
attn_method = "sub"
temperature = 2087
prompt_init = "embedding"
prompt_init_embedding = "soft_prompts/ours/mnli.bin"
prompt_embedding_paths = ["soft_prompts/ours/mnli.bin", "soft_prompts/ours/qnli.bin", "soft_prompts/ours/qqp.bin", "soft_prompts/ours/record.bin", "soft_prompts/ours/squad.bin", "soft_prompts/ours/sst2.bin"]
prefix_num=6
task_type = "seq_2_seq_lm"
num_virtual_tokens = 50
n_runs = 3
wandb_project = "attempt_single_ours_experiments"
max_source_length = 256
split_validation_test = true
# max_train_samples = 1000
# max_valid_samples = 500
# max_test_samples = 500
output_dir = "attempt_single_qqp_ours"
warmup_steps = 500
shared_attn = false

[[configs]]
datasets = ["qnli"]
dataset_config_name = ["en"]
device = "cuda"
learning_rate = 0.3
model_name_or_path = "t5-base"
tokenizer_name_or_path = "t5-base"
max_target_length = 128
num_epochs = 10
batch_size = 32
peft_type = "attempt"
attn_method = "sub"
temperature = 2087
prompt_init = "embedding"
prompt_init_embedding = "soft_prompts/ours/mnli.bin"
prompt_embedding_paths = ["soft_prompts/ours/mnli.bin", "soft_prompts/ours/qnli.bin", "soft_prompts/ours/qqp.bin", "soft_prompts/ours/record.bin", "soft_prompts/ours/squad.bin", "soft_prompts/ours/sst2.bin"]
prefix_num=6
task_type = "seq_2_seq_lm"
num_virtual_tokens = 50
n_runs = 3
wandb_project = "attempt_single_ours_experiments"
max_source_length = 256
split_validation_test = true
# max_train_samples = 1000
# max_valid_samples = 500
# max_test_samples = 500
output_dir = "attempt_single_qnli_ours"
warmup_steps = 500
shared_attn = false

[[configs]]
datasets = ["sst2"]
dataset_config_name = ["en"]
device = "cuda"
learning_rate = 0.3
model_name_or_path = "t5-base"
tokenizer_name_or_path = "t5-base"
max_target_length = 128
num_epochs = 10
batch_size = 32
peft_type = "attempt"
attn_method = "sub"
temperature = 2087
prompt_init = "embedding"
prompt_init_embedding = "soft_prompts/ours/mnli.bin"
prompt_embedding_paths = ["soft_prompts/ours/mnli.bin", "soft_prompts/ours/qnli.bin", "soft_prompts/ours/qqp.bin", "soft_prompts/ours/record.bin", "soft_prompts/ours/squad.bin", "soft_prompts/ours/sst2.bin"]
prefix_num=6
task_type = "seq_2_seq_lm"
num_virtual_tokens = 50
n_runs = 3
wandb_project = "attempt_single_ours_experiments"
max_source_length = 256
split_validation_test = true
# max_train_samples = 1000
# max_valid_samples = 500
# max_test_samples = 500
output_dir = "attempt_single_sst2_ours"
warmup_steps = 500
shared_attn = false

[[configs]]
datasets = ["stsb"]
dataset_config_name = ["en"]
device = "cuda"
learning_rate = 0.3
model_name_or_path = "t5-base"
tokenizer_name_or_path = "t5-base"
max_target_length = 128
num_epochs = 20
batch_size = 32
peft_type = "attempt"
attn_method = "sub"
temperature = 2087
prompt_init = "embedding"
prompt_init_embedding = "soft_prompts/ours/mnli.bin"
prompt_embedding_paths = ["soft_prompts/ours/mnli.bin", "soft_prompts/ours/qnli.bin", "soft_prompts/ours/qqp.bin", "soft_prompts/ours/record.bin", "soft_prompts/ours/squad.bin", "soft_prompts/ours/sst2.bin"]
prefix_num=6
task_type = "seq_2_seq_lm"
num_virtual_tokens = 50
n_runs = 3
wandb_project = "attempt_single_ours_experiments"
max_source_length = 256
split_validation_test = true
# max_train_samples = 1000
# max_valid_samples = 500
# max_test_samples = 500
output_dir = "attempt_single_stsb_ours"
warmup_steps = 500
shared_attn = false

[[configs]]
datasets = ["mrpc"]
dataset_config_name = ["en"]
device = "cuda"
learning_rate = 0.3
model_name_or_path = "t5-base"
tokenizer_name_or_path = "t5-base"
max_target_length = 128
num_epochs = 20
batch_size = 32
peft_type = "attempt"
attn_method = "sub"
temperature = 2087
prompt_init = "embedding"
prompt_init_embedding = "soft_prompts/ours/mnli.bin"
prompt_embedding_paths = ["soft_prompts/ours/mnli.bin", "soft_prompts/ours/qnli.bin", "soft_prompts/ours/qqp.bin", "soft_prompts/ours/record.bin", "soft_prompts/ours/squad.bin", "soft_prompts/ours/sst2.bin"]
prefix_num=6
task_type = "seq_2_seq_lm"
num_virtual_tokens = 50
n_runs = 3
wandb_project = "attempt_single_ours_experiments"
max_source_length = 256
split_validation_test = true
# max_train_samples = 1000
# max_valid_samples = 500
# max_test_samples = 500
output_dir = "attempt_single_mrpc_ours"
warmup_steps = 500
shared_attn = false

[[configs]]
datasets = ["rte"]
dataset_config_name = ["en"]
device = "cuda"
learning_rate = 0.3
model_name_or_path = "t5-base"
tokenizer_name_or_path = "t5-base"
max_target_length = 128
num_epochs = 20
batch_size = 32
peft_type = "attempt"
attn_method = "sub"
temperature = 2087
prompt_init = "embedding"
prompt_init_embedding = "soft_prompts/ours/mnli.bin"
prompt_embedding_paths = ["soft_prompts/ours/mnli.bin", "soft_prompts/ours/qnli.bin", "soft_prompts/ours/qqp.bin", "soft_prompts/ours/record.bin", "soft_prompts/ours/squad.bin", "soft_prompts/ours/sst2.bin"]
prefix_num=6
task_type = "seq_2_seq_lm"
num_virtual_tokens = 50
n_runs = 3
wandb_project = "attempt_single_ours_experiments"
max_source_length = 256
split_validation_test = true
# max_train_samples = 1000
# max_valid_samples = 500
# max_test_samples = 500
output_dir = "attempt_single_rte_ours"
warmup_steps = 500
shared_attn = false

[[configs]]
datasets = ["cola"]
dataset_config_name = ["en"]
device = "cuda"
learning_rate = 0.3
model_name_or_path = "t5-base"
tokenizer_name_or_path = "t5-base"
max_target_length = 128
num_epochs = 20
batch_size = 32
peft_type = "attempt"
attn_method = "sub"
temperature = 2087
prompt_init = "embedding"
prompt_init_embedding = "soft_prompts/ours/mnli.bin"
prompt_embedding_paths = ["soft_prompts/ours/mnli.bin", "soft_prompts/ours/qnli.bin", "soft_prompts/ours/qqp.bin", "soft_prompts/ours/record.bin", "soft_prompts/ours/squad.bin", "soft_prompts/ours/sst2.bin"]
prefix_num=6
task_type = "seq_2_seq_lm"
num_virtual_tokens = 50
n_runs = 3
wandb_project = "attempt_single_ours_experiments"
max_source_length = 256
split_validation_test = true
# max_train_samples = 1000
# max_valid_samples = 500
# max_test_samples = 500
output_dir = "attempt_single_cola_ours"
warmup_steps = 500
shared_attn = false